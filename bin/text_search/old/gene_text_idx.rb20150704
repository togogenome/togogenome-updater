#!/usr/bin/env ruby

require 'json'
require 'fileutils'

ISQL = '/data/store/virtuoso7.1/bin/isql 20711 dba dba'
ISQL_OPT = 'VERBOSE=OFF BANNER=OFF PROMPT=OFF ECHO=OFF BLOBS=ON ERRORS=stderr'
TOGO_DIR = '/data/store/rdf/togogenome/'
BASE_DIR = "#{TOGO_DIR}/bin/text_search"
PREPARE_DIR = "#{TOGO_DIR}/text_search/current/prepare/gene"
OUTPUT_DIR = "#{TOGO_DIR}/text_search/current/gene"
OUTPUT_SOLR_DIR = "#{OUTPUT_DIR}/solr"
TOGO_UP_JSON = "#{PREPARE_DIR}/json/togo_uniprot.json"

#query name
GENE_ATTRIBUTES = 'gene_attributes'
PROTEIN_REFERENCES = 'protein_references'
PROTEIN_CROSS_REFERENCES = 'protein_cross_references'
PROTEIN_SEQUENCE_ANNOTANION = 'protein_sequence_annotation'
PROTEIN_NAMES_GENE_NAMES = 'protein_names_gene_name'
PROTEIN_NAMES_SUMMARY = 'protein_names_summary'
PROTEIN_ONTOLOGIES_KEYWORDS = 'protein_ontologies_keywords'
PROTEIN_ONTOLOGIES_GO = 'protein_ontologies_go'

@metadata = JSON.parse(File.read("#{BASE_DIR}/gene.json"))

def query(query_name)
  STDERR.puts "Start: query [#{query_name}]"
  query_file = "#{BASE_DIR}/sparql/gene/#{query_name}.rq"
  FileUtils.mkdir_p("#{PREPARE_DIR}/text")
  output_file = "#{PREPARE_DIR}/text/#{query_name}.txt"
  system(%Q[#{ISQL} #{ISQL_OPT} < #{query_file} > #{output_file}])
  STDERR.puts "End: query [#{query_name}]"
end

def create_prepare_json(query_name)
  STDERR.puts "Start: create prepare json [#{query_name}]"
  input_file = "#{PREPARE_DIR}/text/#{query_name}.txt"
  FileUtils.mkdir_p("#{PREPARE_DIR}/json")
  output_file  = "#{PREPARE_DIR}/json/#{query_name}.json" 
  File.open("#{input_file}") do |f|
    result_hash = {}
    while line  = f.gets
      if query_name.start_with?('gene')
        gene_obj_mapping(result_hash, line, query_name)
      elsif query_name.start_with?('protein')
        protein_obj_mapping(result_hash, line, query_name)
      end
    end
    File.open("#{output_file}", 'w') do |file|
      file.puts JSON.pretty_generate(result_hash)
    end
  end
  STDERR.puts "End: create prepare json [#{query_name}]"
end

def to_utf(str)
  str.force_encoding('UTF-8')
end

def gene_obj_mapping(result_hash, line, query_name)
  return line.start_with?('http://togogenome.org/gene/') unless
  columns = line.split('^@')
  gene_id = columns[0].strip.gsub('http://togogenome.org/gene/','')

  case query_name
  when GENE_ATTRIBUTES
    values = { :gene_id => to_utf(gene_id),
               :locus_tags => to_utf(columns[1].strip),
               :gene_names => to_utf(columns[2].strip),
               :sequence_labels => to_utf(columns[3].strip),
               :refseq_labels => to_utf(columns[4].strip),
               :sequence_organism_names => to_utf(columns[5].strip)}
  end
  result_hash[gene_id] = values
end

def protein_obj_mapping(result_hash, line, query_name)
  return line.start_with?('http://purl.uniprot.org/uniprot/') unless
  columns = line.split('^@')
  uniprot_no = columns[0].strip.gsub('http://purl.uniprot.org/uniprot/','')

  case query_name
  when PROTEIN_CROSS_REFERENCES
    xref_ids = columns[3].strip.split('||').map do |uri|
      uri.split('/').last
    end
    values = { :uniprot_id => to_utf(uniprot_no),
               :up_xref_categories => to_utf(columns[1].strip),
               :up_xref_abbrs => to_utf(columns[2].strip),
               :up_xref_ids => to_utf(xref_ids.join(','))}
  when PROTEIN_REFERENCES
    values = { :uniprot_id => to_utf(uniprot_no),
               :up_ref_pubmed_ids => to_utf(columns[1].strip),
               :up_ref_citation_names => to_utf(columns[2].strip),
               :up_ref_citation_titles => to_utf(columns[3].strip),
               :up_ref_citation_authors => to_utf(columns[4].strip)}
  when PROTEIN_SEQUENCE_ANNOTANION
    feature_ids = columns[4].strip.gsub('http://purl.uniprot.org/annotation/','')
    values = { :uniprot_id => to_utf(uniprot_no),
               :up_seq_anno_parent_labels => to_utf(columns[1].strip),
               :up_seq_anno_labels => to_utf(columns[2].strip),
               :up_seq_anno_comments => to_utf(columns[3].strip),
               :up_seq_anno_feature_ids => to_utf(feature_ids) }
  when PROTEIN_NAMES_GENE_NAMES
    values = { :uniprot_id => to_utf(uniprot_no),
               :up_gene_names => to_utf(columns[1].strip),
               :up_synonyms_names => to_utf(columns[2].strip),
               :up_locus_tags => to_utf(columns[3].strip),
               :up_orf_names => to_utf(columns[4].strip)}
  when PROTEIN_NAMES_SUMMARY
    values = { :uniprot_id => to_utf(uniprot_no),
               :up_recommended_names => to_utf(columns[1].strip),
               :up_ec_names => to_utf(columns[2].strip),
               :up_alternative_names => to_utf(columns[3].strip)}
  when PROTEIN_ONTOLOGIES_KEYWORDS
    values = { :uniprot_id => to_utf(uniprot_no),
               :up_keyword_root_names => to_utf(columns[1].strip),
               :up_keyword_names => to_utf(columns[2].strip)}
  when PROTEIN_ONTOLOGIES_GO
    values = { :uniprot_id => to_utf(uniprot_no),
               :up_go_names => to_utf(columns[1].strip)}
  end
  result_hash[uniprot_no] = values
end

def cretate_idx_from_gene (stanza_name, query_names)
  FileUtils.mkdir_p("#{OUTPUT_DIR}")
  FileUtils.mkdir_p("#{OUTPUT_SOLR_DIR}")
  output_file  = "#{OUTPUT_DIR}/#{stanza_name}.jsonld"  
  output_solr_file  = "#{OUTPUT_SOLR_DIR}/#{stanza_name}.json"
  if @gene_up_map == nil
    @gene_up_map = JSON.parse(File.read("#{TOGO_UP_JSON}"))
    STDERR.puts('End: create gene up map')
    gene_id_list = @gene_up_map.keys
    STDERR.puts('End: create gene id list')
  end
  
  result = {}
  query_names.each do |query_name|
    gene_text_data = JSON.parse(File.read("#{PREPARE_DIR}/json/#{query_name}.json"))
    STDERR.puts("End: loaded gene data [#{query_name}]")
    gene_id_list.each do |gene_id|
      next if gene_text_data[gene_id].nil? # skip if query data has no current gene id
      if result[gene_id] == nil
        result[gene_id] = gene_text_data[gene_id]
      else # current gene id has already added gene value
        result[gene_id].merge!(gene_text_data[gene_id]) do |key, oldval, newval|
          if key == 'gene_id' && oldval.split(',').include?(newval) # no duplicate of gene id
            oldval
          else # concat text data
            oldval + ',' + newval
          end
        end
      end
      gene_text_data.delete(gene_id)
    end
    STDERR.puts("End: create index data [#{query_name}]")
  end

  columns = []
  @metadata["stanzas"].map do |stanza|
    if stanza_name == stanza["stanza_name"]
      stanza["queries"].each do |query|
        query["columns"].each do |column|
          columns.push(column["columns_name"])
        end
      end
    end
  end
  columns = columns.uniq

  STDERR.puts('Start: output index file')
  solr_index_file = File.open("#{output_solr_file}", 'w')
  File.open("#{output_file}", 'w') do |file|
    file.puts '{'
    file.puts '"@context" :'
    file.puts JSON.pretty_generate(get_context_hash(stanza_name, columns))
    file.puts ','
    file.puts '"@graph" :'
    file.puts '['
    solr_index_file.puts '['
    comma = ','
    cnt = 0
    result.each do |gene_id, gene_info|
      if cnt == result.size - 1
        comma = ''
      end
      #hash = {"@id" => gene_id, "values" => gene_info}
      hash = {"@id" => "http://togogenome.org/gene/#{gene_id}"}.merge(gene_info)
      file.puts JSON.pretty_generate(hash) + comma
      solr_index_file.puts JSON.pretty_generate(hash) + comma
      cnt += 1
    end
    file.puts ']'
    solr_index_file.puts ']'
    file.puts '}'
  end
end

def cretate_idx_from_protein (stanza_name, query_names)
  FileUtils.mkdir_p("#{OUTPUT_DIR}")
  FileUtils.mkdir_p("#{OUTPUT_SOLR_DIR}")
  output_file  = "#{OUTPUT_DIR}/#{stanza_name}.json"  
  output_solr_file  = "#{OUTPUT_SOLR_DIR}/#{stanza_name}.json"
  if @gene_up_map == nil
    @gene_up_map = JSON.parse(File.read("#{TOGO_UP_JSON}"))
  end
  gene_id_list = @gene_up_map.keys
  result = {}
  query_names.each do |query_name|
    up_text_data = JSON.parse(File.read("#{PREPARE_DIR}/json/#{query_name}.json"))
    STDERR.puts("End: loaded protein data [#{query_name}]")
    gene_id_list.each do |gene_id|
      @gene_up_map[gene_id].each do |uniprot_id| # gene id has linked multi uniprot id
        next if up_text_data[uniprot_id].nil? # skip if query data has no current uniprot id
        if result[gene_id] == nil
          up_text_data[uniprot_id]["gene_id"] = gene_id
          result[gene_id] = up_text_data[uniprot_id]
        else # current gene id has already added protein value
          result[gene_id].merge!(up_text_data[uniprot_id]) do |key, oldval, newval|
            if key == 'uniprot_id' && oldval.split(',').include?(newval) # no duplicate of uniprot id
              oldval
            else # concat text data
              oldval + ',' + newval
            end
          end
        end
        up_text_data.delete(uniprot_id)
      end
    end
    STDERR.puts("End: create index data [#{query_name}]")
  end

  columns = []
  @metadata["stanzas"].map do |stanza|
    if stanza_name == stanza["stanza_name"]
      stanza["queries"].each do |query|
        query["columns"].each do |column|
          columns.push(column["columns_name"])
        end
      end
    end
  end
  columns = columns.uniq

  STDERR.puts('Start: output index file')
  solr_index_file = File.open("#{output_solr_file}", 'w')
  File.open("#{output_file}", 'w') do |file|
    file.puts '{'
    file.puts '"@context" :'
    file.puts JSON.pretty_generate(get_context_hash(stanza_name, columns))
    file.puts ','
    file.puts '"@graph" :'
    file.puts '['
    solr_index_file.puts '['
    comma = ','
    cnt = 0
    result.each do |gene_id, protein_info|
      if cnt == result.size - 1 
        comma = ''
      end
     # hash = {"@id" => gene_id, "values" => protein_info}
      hash = {"@id" => "http://togogenome.org/gene/#{gene_id}"}.merge(protein_info)
      file.puts JSON.pretty_generate(hash) + comma
      solr_index_file.puts JSON.pretty_generate(hash) + comma
      cnt += 1
    end
    file.puts ']'
    solr_index_file.puts ']'
    file.puts '}'
  end
  solr_index_file.close()
end

def get_context_hash(stanza_name, column_names)
  base_url = "http://togogenome.org/#{stanza_name}"

  hash = {}
  column_names.each do |column_name|
    hash[column_name] = "#{base_url}/#{column_name}"
  end
  if hash["gene_id"] == nil
    hash["gene_id"] = "#{base_url}/gene_id"
  end
  hash
end


@metadata["stanzas"].each do |stanza|
   puts "stanza_name:" + stanza["stanza_name"]
   stanza["queries"].each do |query|
     puts " query_name:" + query["query_name"]
   end  
end

#query(GENE_ATTRIBUTES)
#create_prepare_json(GENE_ATTRIBUTES)
#Time: 925s + 12316s 3h 40min

#query(PROTEIN_CROSS_REFERENCES)
#create_prepare_json(PROTEIN_CROSS_REFERENCES)
#Time: 5843s + 766s 1h 50min

#query(PROTEIN_REFERENCES)
#create_prepare_json(PROTEIN_REFERENCES)
#Time: 1h 45min

#query(PROTEIN_NAMES_GENE_NAMES)
#create_prepare_json(PROTEIN_NAMES_GENE_NAMES)
#Time: 494s + 589s 18min

#query(PROTEIN_NAMES_SUMMARY)
#create_prepare_json(PROTEIN_NAMES_SUMMARY)
#Time: 15min 

#query(PROTEIN_SEQUENCE_ANNOTANION)
#create_prepare_json(PROTEIN_SEQUENCE_ANNOTANION)
#Time: 12113s + 843s 3h 36min 

#query(PROTEIN_ONTOLOGIES_KEYWORDS)
#create_prepare_json(PROTEIN_ONTOLOGIES_KEYWORDS)
#Time: 766s + 4003s 1h 20min

query(PROTEIN_ONTOLOGIES_GO)
#create_prepare_json(PROTEIN_ONTOLOGIES_GO)
#Time: 282s + 209s 8min


### Create index file ###
#query_names = [GENE_ATTRIBUTES]
#cretate_idx_from_gene('gene_attributes', query_names)
# min mem G

#query_names = [PROTEIN_CROSS_REFERENCES]
#cretate_idx_from_protein('protein_cross_references', query_names)
# 8min mem 422G

#query_names = [PROTEIN_REFERENCES]
#cretate_idx_from_protein('protein_references', query_names)
# 8min mem 34G

#query_names = [PROTEIN_NAMES_GENE_NAMES, PROTEIN_NAMES_SUMMARY]
#cretate_idx_from_protein('protein_names', query_names)
# 18min mem 47G

#query_names = [PROTEIN_SEQUENCE_ANNOTANION]
#cretate_idx_from_protein('protein_sequence_annotation', query_names)
# 2min mem G

#query_names = [PROTEIN_ONTOLOGIES_KEYWORDS, PROTEIN_ONTOLOGIES_GO]
#cretate_idx_from_protein('protein_ontologies', query_names)
# 15min mem 29G
